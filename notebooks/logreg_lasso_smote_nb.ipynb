{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSIS QUESTIONS**\n",
    "\n",
    "1. Discover relationship between Total Complexity Points (as of May 2021) and subsequent PRODUCTION, OUTFLOW, and NET. 9 months is probably preferred, but explore whether there are differences between 3 and 6 months.\n",
    "\n",
    "2. Same as above, but for each individual Complexity input   (also, including the data on \"Other Data\" tab as separate variables).\n",
    "\n",
    "3. Create simple model that tries to predict which clients will contribute over $1,000,000 in production in 9 months based on Complexity data only. Is there a \"formula\" for complexity that indicates that the client WILL, WON'T ever contribute significant production?\n",
    "\n",
    "4. Explore link between Complexity Points and # of Meetings, Zooms, calls in subsequent 9 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set view options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in excel spreadsheets\n",
    "complexity = pd.read_excel('../data/RohitCapstoneDataApr2022.xlsx', sheet_name = 'Complexity Data') \\\n",
    "    .rename(columns = {'rel_id': 'RelID'})\n",
    "\n",
    "prod_outflow_columns = {\n",
    "    'rel_id': 'RelID', \n",
    "    'CurrentRelValue': 'CurrentValue',\n",
    "    'Beginning Rel Value': 'BeginningValue',\n",
    "    'ThreeMonthProd': '3MonthProd',\n",
    "    'SixMonthProd': '6MonthProd',\n",
    "    'NineMonthProd': '9MonthProd',\n",
    "    'ThreeMonthOutflow': '3MonthOutflow',\n",
    "    'SixMonthOutflow': '6MonthOutflow',\n",
    "    'NineMonthOutflow': '9MonthOutflow'\n",
    "}\n",
    "prod_outflow = pd.read_excel('../data/RohitCapstoneDataApr2022.xlsx', sheet_name = 'Prod-Outflow Data', skiprows = 1) \\\n",
    "    .rename(columns = prod_outflow_columns)[prod_outflow_columns.values()]\n",
    "\n",
    "other = pd.read_excel('../data/RohitCapstoneDataApr2022.xlsx', sheet_name = 'Other Data as of 202105') \\\n",
    "    .drop(columns = 'AsOf')\n",
    "\n",
    "meetings = pd.read_excel('../data/RohitCapstoneDataApr2022.xlsx', sheet_name = 'Subsequent Mtgs', converters = {'AsOf': str})\n",
    "date_filter = ['202106', '202107', '202108', '202109', '202110', '202111', '202112', '202201', '202202', '202203', '202204']\n",
    "meetings = meetings.loc[meetings['AsOf'].isin(date_filter)]\n",
    "meetings['Year'] = meetings['AsOf'].str[0:4]\n",
    "meetings['Month'] = meetings['AsOf'].str[4:6]\n",
    "meetings['Interactions'] = meetings['Call'] + meetings['Meeting'] + meetings['Zoom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show complexity rules\n",
    "complexity_table = complexity[['RuleID', 'RuleType', '# Pts', 'Name']].sort_values('RuleID').drop_duplicates().reset_index(drop = True)\n",
    "complexity_table = complexity_table.merge(complexity.value_counts('RuleID').to_frame('# RelIDs'), on = 'RuleID')\n",
    "complexity_table\n",
    "# complexity_table.style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector = 'th', props=[('text-align', 'left')])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter accounts with BeginningValue less than $500,000\n",
    "prod_outflow = prod_outflow[prod_outflow['BeginningValue'] >= 500000]\n",
    "\n",
    "# Calculate net, net percentage, and percentage change variables\n",
    "prod_outflow['3MonthNet'] = prod_outflow['3MonthProd'] + prod_outflow['3MonthOutflow']\n",
    "prod_outflow['6MonthNet'] = prod_outflow['6MonthProd'] + prod_outflow['6MonthOutflow']\n",
    "prod_outflow['9MonthNet'] = prod_outflow['9MonthProd'] + prod_outflow['9MonthOutflow']\n",
    "\n",
    "prod_outflow['3MonthNetPct'] = prod_outflow['3MonthNet'] / prod_outflow['BeginningValue'] * 100\n",
    "prod_outflow['6MonthNetPct'] = prod_outflow['6MonthNet'] / prod_outflow['BeginningValue'] * 100\n",
    "prod_outflow['9MonthNetPct'] = prod_outflow['9MonthNet'] / prod_outflow['BeginningValue'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up complexity points per RelID\n",
    "complexity_pts = complexity.groupby('RelID', as_index = False)['# Pts'].sum()\n",
    "\n",
    "# Sum up client interactions per RelID\n",
    "interactions = meetings.loc[meetings['CategoryName'] == 'Client Review'].groupby(['RelID'], as_index = False)['Interactions'].sum()\n",
    "\n",
    "# Merge complexity_pts, prod_outflow, other, and interactions data\n",
    "pts_prod_outflow = complexity_pts.merge(prod_outflow, on = 'RelID').set_index('RelID')\n",
    "pts_prod_outflow_other = pts_prod_outflow.merge(other, on = 'RelID').set_index('RelID')\n",
    "pts_prod_outflow_other_interactions = pts_prod_outflow_other.merge(interactions, how = 'left', on = 'RelID').set_index('RelID').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up complexity rule dummy variables per RelID\n",
    "complexity_rules = pd.get_dummies(complexity.set_index('RelID')['ComplexityRuleID'].astype(str)).groupby('RelID').sum().reset_index()\n",
    "\n",
    "# Create ordinal complexity rule variables\n",
    "complexity_rules['2-3'] = complexity_rules['2'] * 1 + complexity_rules['3'] * 2\n",
    "complexity_rules['4-5'] = complexity_rules['4'] * 1 + complexity_rules['5'] * 2\n",
    "complexity_rules['6-7'] = complexity_rules['6'] * 1 + complexity_rules['7'] * 2\n",
    "complexity_rules['8-9'] = complexity_rules['8'] * 1 + complexity_rules['9'] * 2\n",
    "complexity_rules['10-13'] = complexity_rules['10'] * 1 + complexity_rules['11'] * 2 + complexity_rules['12'] * 3 + complexity_rules['13'] * 4\n",
    "complexity_rules['14-16'] = complexity_rules['14'] * 1 + complexity_rules['15'] * 2 + complexity_rules['16'] * 3\n",
    "complexity_rules['41-43'] = complexity_rules['41'] * 1 + complexity_rules['42'] * 2 + complexity_rules['43'] * 3\n",
    "complexity_rules['49-51'] = complexity_rules['49'] * 1 + complexity_rules['50'] * 2 + complexity_rules['51'] * 3\n",
    "complexity_rules['52-53'] = complexity_rules['52'] * 1 + complexity_rules['53'] * 2\n",
    "complexity_rules['65'] = 1 - complexity_rules[['4', '5', '60']].sum(axis = 1)\n",
    "\n",
    "# Merge complexity_rules, prod_outflow, other, and interactions data\n",
    "rules_prod_outflow = complexity_rules.merge(prod_outflow, on = 'RelID').set_index('RelID')\n",
    "rules_prod_outflow_other = rules_prod_outflow.merge(other, on = 'RelID').set_index('RelID')\n",
    "rules_prod_outflow_other_interactions = rules_prod_outflow_other.merge(interactions, how = 'left', on = 'RelID').set_index('RelID').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "all_rules = [\n",
    "    '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15',\n",
    "    '16', '17', '18', '19', '20', '21', '22', '23', '24', '26', '27', '28', '29', '30',\n",
    "    '31', '32', '33', '37', '38', '39', '40', '41', '42', '43', '45', '49', '50', '51',\n",
    "    '52', '53', '54', '55', '56', '57', '58', '60', '62', '63', '64'\n",
    "]\n",
    "\n",
    "collapsed_rules = [\n",
    "    '1', '2-3', '4-5', '6-7', '8-9', '10-13', '14-16', '17', '18', '19',\n",
    "    '20', '21', '22', '23', '24', '26', '27', '28', '29', '30', '31', \n",
    "    '32', '33', '37', '38', '39', '40', '41-43', '45', '49-51', '52-53',\n",
    "    '54', '55', '56', '57', '58', '62', '63', '65'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic LASSO SMOTE pipeline\n",
    "lr_l1_smote_pipeline = Pipeline(\n",
    "    steps = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('vt', VarianceThreshold()),\n",
    "        ('logistic', LogisticRegression(penalty = 'l1', class_weight = 'balanced', solver = 'saga', max_iter = 1000))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train/test split (all complexity rules)\n",
    "X = rules_prod_outflow_other_interactions\n",
    "y = (rules_prod_outflow_other_interactions['9MonthProd'] >= 1000000).astype(int)\n",
    "X_train_all_rules, X_test_all_rules, y_train_all_rules, y_test_all_rules = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 321)\n",
    "\n",
    "# Oversample using SMOTE, then randomly undersample\n",
    "X_train_all_rules, y_train_all_rules = SMOTEN(sampling_strategy = 0.1, k_neighbors = 5, random_state = 321).fit_resample(X_train_all_rules[all_rules], y_train_all_rules)\n",
    "X_train_all_rules, y_train_all_rules = RandomUnderSampler(sampling_strategy = 0.55, random_state = 321).fit_resample(X_train_all_rules, y_train_all_rules)\n",
    "\n",
    "# Run grid search using logistic LASSO SMOTE pipeline\n",
    "grid_search_smote_all_rules = GridSearchCV(estimator = lr_l1_smote_pipeline, param_grid = {'logistic__C': [1, 0.5, 0.1, 0.05, 0.01]}, scoring = 'f1', cv = 3)\n",
    "grid_search_smote_all_rules.fit(X_train_all_rules, y_train_all_rules)\n",
    "grid_search_smote_all_rules.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output logistic LASSO SMOTE model results (all complexity rules)\n",
    "print(classification_report(y_test_all_rules, grid_search_smote_all_rules.best_estimator_.predict(X_test_all_rules[all_rules]), zero_division = 0))\n",
    "plt.show(ConfusionMatrixDisplay.from_estimator(grid_search_smote_all_rules.best_estimator_, X_test_all_rules[all_rules], y_test_all_rules))\n",
    "\n",
    "ventiles_df = pd.DataFrame({\n",
    "    'Probability': grid_search_smote_all_rules.best_estimator_.predict_proba(X_test_all_rules[all_rules])[:, 1],\n",
    "    'Avg Prod': X_test_all_rules['9MonthProd'],\n",
    "    'Median Prod': X_test_all_rules['9MonthProd'],\n",
    "    '$1M+ Prod': y_test_all_rules,\n",
    "    '$1M+ Prod %': y_test_all_rules\n",
    "})\n",
    "\n",
    "agg_dict = {\n",
    "    'Avg Prod': 'mean',\n",
    "    'Median Prod': 'median',\n",
    "    '$1M+ Prod': 'sum',\n",
    "    '$1M+ Prod %': 'sum',\n",
    "    'Probability': 'mean',\n",
    "}\n",
    "\n",
    "ventiles_df['Interval'] = pd.qcut(ventiles_df['Probability'], q = 20, labels = [number for number in range(1, 21)], precision = 0)\n",
    "ventiles_df['Ventile'] = pd.qcut(ventiles_df['Probability'], q = 20, precision = 0)\n",
    "ventiles_df = ventiles_df.groupby(['Interval', 'Ventile']).agg(agg_dict).dropna()\n",
    "ventiles_df['Odds'] = ventiles_df['Probability'] / (1 - ventiles_df['Probability'])\n",
    "ventiles_df['$1M+ Prod %'] = ventiles_df['$1M+ Prod'][::-1].cumsum()[::-1] / ventiles_df['$1M+ Prod'].sum() * 100\n",
    "display(ventiles_df)\n",
    "\n",
    "coefficients = pd.DataFrame({\n",
    "    'variable': ['intercept'] + list(np.array(X_train_all_rules[all_rules].columns)[grid_search_smote_all_rules.best_estimator_['vt'].get_support()]),\n",
    "    'coefficient': [grid_search_smote_all_rules.best_estimator_['logistic'].intercept_[0]] + list(grid_search_smote_all_rules.best_estimator_['logistic'].coef_[0])\n",
    "}).merge(complexity_table[['Name', 'ComplexityRuleID']].astype(str).set_index('ComplexityRuleID'), how = 'left', left_on = 'variable', right_on = 'ComplexityRuleID')\n",
    "display(coefficients.sort_values('coefficient', ascending = False, key = abs).fillna('').head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train/test split (all complexity rules, indeterminate set filter)\n",
    "X = rules_prod_outflow_other_interactions\n",
    "y = (rules_prod_outflow_other_interactions['9MonthProd'] >= 1000000).astype(int)\n",
    "X_train_all_rules_indeterminate_set, X_test_all_rules_indeterminate_set, y_train_all_rules_indeterminate_set, y_test_all_rules_indeterminate_set = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 321)\n",
    "\n",
    "# Filter out indeterminate set\n",
    "indeterminate_filter = X_train_all_rules_indeterminate_set.loc[~X_train_all_rules_indeterminate_set['9MonthProd'].between(250000, 1000000)].index\n",
    "X_train_all_rules_indeterminate_set, y_train_all_rules_indeterminate_set = X_train_all_rules_indeterminate_set.loc[indeterminate_filter], y_train_all_rules_indeterminate_set.loc[indeterminate_filter]\n",
    "\n",
    "# Oversample using SMOTE, then randomly undersample\n",
    "X_train_smote_all_rules_indeterminate_set, y_train_all_rules_indeterminate_set = SMOTEN(sampling_strategy = 0.1, k_neighbors = 5, random_state = 321).fit_resample(X_train_all_rules_indeterminate_set[all_rules], y_train_all_rules_indeterminate_set)\n",
    "X_train_all_rules_indeterminate_set, y_train_all_rules_indeterminate_set = RandomUnderSampler(sampling_strategy = 0.55, random_state = 321).fit_resample(X_train_smote_all_rules_indeterminate_set, y_train_all_rules_indeterminate_set)\n",
    "\n",
    "# Run grid search using logistic LASSO SMOTE pipeline\n",
    "grid_search_smote_all_rules_indeterminate_set = GridSearchCV(estimator = lr_l1_smote_pipeline, param_grid = {'logistic__C': [1, 0.5, 0.1, 0.05, 0.01]}, scoring = 'f1', cv = 3)\n",
    "grid_search_smote_all_rules_indeterminate_set.fit(X_train_all_rules_indeterminate_set, y_train_all_rules_indeterminate_set)\n",
    "grid_search_smote_all_rules_indeterminate_set.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output logistic LASSO SMOTE model results (all complexity rules, indeterminate set filter)\n",
    "print(classification_report(y_test_all_rules_indeterminate_set, grid_search_smote_all_rules.best_estimator_.predict(X_test_all_rules_indeterminate_set[all_rules]), zero_division = 0))\n",
    "plt.show(ConfusionMatrixDisplay.from_estimator(grid_search_smote_all_rules.best_estimator_, X_test_all_rules_indeterminate_set[all_rules], y_test_all_rules_indeterminate_set))\n",
    "\n",
    "ventiles_df = pd.DataFrame({\n",
    "    'Probability': grid_search_smote_all_rules.best_estimator_.predict_proba(X_test_all_rules_indeterminate_set[all_rules])[:, 1],\n",
    "    'Avg Prod': X_test_all_rules_indeterminate_set['9MonthProd'],\n",
    "    'Median Prod': X_test_all_rules_indeterminate_set['9MonthProd'],\n",
    "    '$1M+ Prod': y_test_all_rules_indeterminate_set,\n",
    "    '$1M+ Prod %': y_test_all_rules_indeterminate_set\n",
    "})\n",
    "\n",
    "agg_dict = {\n",
    "    'Avg Prod': 'mean',\n",
    "    'Median Prod': 'median',\n",
    "    '$1M+ Prod': 'sum',\n",
    "    '$1M+ Prod %': 'sum',\n",
    "    'Probability': 'mean',\n",
    "}\n",
    "\n",
    "ventiles_df['Interval'] = pd.qcut(ventiles_df['Probability'], q = 20, labels = [number for number in range(1, 21)], precision = 0)\n",
    "ventiles_df['Ventile'] = pd.qcut(ventiles_df['Probability'], q = 20, precision = 0)\n",
    "ventiles_df = ventiles_df.groupby(['Interval', 'Ventile']).agg(agg_dict).dropna()\n",
    "ventiles_df['Odds'] = ventiles_df['Probability'] / (1 - ventiles_df['Probability'])\n",
    "ventiles_df['$1M+ Prod %'] = ventiles_df['$1M+ Prod'][::-1].cumsum()[::-1] / ventiles_df['$1M+ Prod'].sum() * 100\n",
    "display(ventiles_df)\n",
    "\n",
    "coefficients = pd.DataFrame({\n",
    "    'variable': ['intercept'] + list(np.array(X_train_all_rules_indeterminate_set[all_rules].columns)[grid_search_smote_all_rules.best_estimator_['vt'].get_support()]),\n",
    "    'coefficient': [grid_search_smote_all_rules.best_estimator_['logistic'].intercept_[0]] + list(grid_search_smote_all_rules.best_estimator_['logistic'].coef_[0])\n",
    "}).merge(complexity_table[['Name', 'ComplexityRuleID']].astype(str).set_index('ComplexityRuleID'), how = 'left', left_on = 'variable', right_on = 'ComplexityRuleID')\n",
    "display(coefficients.sort_values('coefficient', ascending = False, key = abs).fillna('').head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train/test split (collapsed complexity rules)\n",
    "X = rules_prod_outflow_other_interactions\n",
    "y = (rules_prod_outflow_other_interactions['9MonthProd'] >= 1000000).astype(int)\n",
    "X_train_collapsed_rules, X_test_collapsed_rules, y_train_collapsed_rules, y_test_collapsed_rules = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 321)\n",
    "\n",
    "# Oversample using SMOTE, then randomly undersample\n",
    "X_train_smote_collapsed_rules, y_train_smote_collapsed_rules = SMOTEN(sampling_strategy = 0.1, k_neighbors = 5, random_state = 321).fit_resample(X_train_collapsed_rules[collapsed_rules], y_train_collapsed_rules)\n",
    "X_train_final_collapsed_rules, y_train_final_collapsed_rules = RandomUnderSampler(sampling_strategy = 0.55, random_state = 321).fit_resample(X_train_smote_collapsed_rules, y_train_smote_collapsed_rules)\n",
    "\n",
    "# Run grid search using logistic LASSO SMOTE pipeline\n",
    "grid_search_smote_collapsed_rules = GridSearchCV(estimator = lr_l1_smote_pipeline, param_grid = {'logistic__C': [1, 0.5, 0.1, 0.05, 0.01]}, scoring = 'f1', cv = 3)\n",
    "grid_search_smote_collapsed_rules.fit(X_train_final_collapsed_rules, y_train_final_collapsed_rules)\n",
    "grid_search_smote_collapsed_rules.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output logistic LASSO SMOTE model results (collapsed complexity rules)\n",
    "print(classification_report(y_test_collapsed_rules, grid_search_smote_collapsed_rules.best_estimator_.predict(X_test_collapsed_rules[collapsed_rules]), zero_division = 0))\n",
    "plt.show(ConfusionMatrixDisplay.from_estimator(grid_search_smote_collapsed_rules.best_estimator_, X_test_collapsed_rules[collapsed_rules], y_test_collapsed_rules))\n",
    "\n",
    "ventiles_df = pd.DataFrame({\n",
    "    'Probability': grid_search_smote_collapsed_rules.best_estimator_.predict_proba(X_test_collapsed_rules[collapsed_rules])[:, 1],\n",
    "    'Avg Prod': X_test_collapsed_rules['9MonthProd'],\n",
    "    'Median Prod': X_test_collapsed_rules['9MonthProd'],\n",
    "    '$1M+ Prod': y_test_collapsed_rules,\n",
    "    '$1M+ Prod %': y_test_collapsed_rules\n",
    "})\n",
    "\n",
    "agg_dict = {\n",
    "    'Avg Prod': 'mean',\n",
    "    'Median Prod': 'median',\n",
    "    '$1M+ Prod': 'sum',\n",
    "    '$1M+ Prod %': 'sum',\n",
    "    'Probability': 'mean',\n",
    "}\n",
    "\n",
    "ventiles_df['Interval'] = pd.qcut(ventiles_df['Probability'], q = 20, labels = [number for number in range(1, 21)], precision = 0)\n",
    "ventiles_df['Ventile'] = pd.qcut(ventiles_df['Probability'], q = 20, precision = 0)\n",
    "ventiles_df = ventiles_df.groupby(['Interval', 'Ventile']).agg(agg_dict).dropna()\n",
    "ventiles_df['Odds'] = ventiles_df['Probability'] / (1 - ventiles_df['Probability'])\n",
    "ventiles_df['$1M+ Prod %'] = ventiles_df['$1M+ Prod'][::-1].cumsum()[::-1] / ventiles_df['$1M+ Prod'].sum() * 100\n",
    "display(ventiles_df)\n",
    "\n",
    "coefficients = pd.DataFrame({\n",
    "    'variable': ['intercept'] + list(np.array(X_train_collapsed_rules[collapsed_rules].columns)[grid_search_smote_collapsed_rules.best_estimator_['vt'].get_support()]),\n",
    "    'coefficient': [grid_search_smote_collapsed_rules.best_estimator_['logistic'].intercept_[0]] + list(grid_search_smote_collapsed_rules.best_estimator_['logistic'].coef_[0])\n",
    "}).merge(complexity_table[['Name', 'ComplexityRuleID']].astype(str).set_index('ComplexityRuleID'), how = 'left', left_on = 'variable', right_on = 'ComplexityRuleID')\n",
    "display(coefficients.sort_values('coefficient', ascending = False, key = abs).fillna('').head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train/test split (collapsed complexity rules, indeterminate set filter)\n",
    "X = rules_prod_outflow_other_interactions\n",
    "y = (rules_prod_outflow_other_interactions['9MonthProd'] >= 1000000).astype(int)\n",
    "X_train_collapsed_rules_indeterminate_set, X_test_collapsed_rules_indeterminate_set, y_train_collapsed_rules_indeterminate_set, y_test_collapsed_rules_indeterminate_set = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 321)\n",
    "\n",
    "# Filter out indeterminate set\n",
    "indeterminate_filter = X_train_collapsed_rules_indeterminate_set.loc[~X_train_collapsed_rules_indeterminate_set['9MonthProd'].between(250000, 1000000)].index\n",
    "X_train_collapsed_rules_indeterminate_set, y_train_collapsed_rules_indeterminate_set = X_train_collapsed_rules_indeterminate_set.loc[indeterminate_filter], y_train_collapsed_rules_indeterminate_set.loc[indeterminate_filter]\n",
    "\n",
    "# Oversample using SMOTE, then randomly undersample\n",
    "X_train_smote_collapsed_rules_indeterminate_set, y_train_smote_collapsed_rules_indeterminate_set = SMOTEN(sampling_strategy = 0.1, k_neighbors = 5, random_state = 321).fit_resample(X_train_collapsed_rules_indeterminate_set[collapsed_rules], y_train_collapsed_rules_indeterminate_set)\n",
    "X_train_final_collapsed_rules_indeterminate_set, y_train_final_collapsed_rules_indeterminate_set = RandomUnderSampler(sampling_strategy = 0.55, random_state = 321).fit_resample(X_train_smote_collapsed_rules_indeterminate_set, y_train_smote_collapsed_rules_indeterminate_set)\n",
    "\n",
    "# Run grid search using logistic LASSO SMOTE pipeline\n",
    "grid_search_smote_collapsed_rules_indeterminate_set = GridSearchCV(estimator = lr_l1_smote_pipeline, param_grid = {'logistic__C': [1, 0.5, 0.1, 0.05, 0.01]}, scoring = 'f1', cv = 3)\n",
    "grid_search_smote_collapsed_rules_indeterminate_set.fit(X_train_final_collapsed_rules_indeterminate_set, y_train_final_collapsed_rules_indeterminate_set)\n",
    "grid_search_smote_collapsed_rules_indeterminate_set.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output logistic LASSO SMOTE model results (collapsed complexity rules, indeterminate set filter)\n",
    "print(classification_report(y_test_collapsed_rules_indeterminate_set, grid_search_smote_collapsed_rules_indeterminate_set.best_estimator_.predict(X_test_collapsed_rules_indeterminate_set[collapsed_rules]), zero_division = 0))\n",
    "plt.show(ConfusionMatrixDisplay.from_estimator(grid_search_smote_collapsed_rules_indeterminate_set.best_estimator_, X_test_collapsed_rules_indeterminate_set[collapsed_rules], y_test_collapsed_rules_indeterminate_set))\n",
    "\n",
    "ventiles_df = pd.DataFrame({\n",
    "    'Probability': grid_search_smote_collapsed_rules_indeterminate_set.best_estimator_.predict_proba(X_test_collapsed_rules_indeterminate_set[collapsed_rules])[:, 1],\n",
    "    'Avg Prod': X_test_collapsed_rules_indeterminate_set['9MonthProd'],\n",
    "    'Median Prod': X_test_collapsed_rules_indeterminate_set['9MonthProd'],\n",
    "    '$1M+ Prod': y_test_collapsed_rules_indeterminate_set,\n",
    "    '$1M+ Prod %': y_test_collapsed_rules_indeterminate_set\n",
    "})\n",
    "\n",
    "agg_dict = {\n",
    "    'Avg Prod': 'mean',\n",
    "    'Median Prod': 'median',\n",
    "    '$1M+ Prod': 'sum',\n",
    "    '$1M+ Prod %': 'sum',\n",
    "    'Probability': 'mean',\n",
    "}\n",
    "\n",
    "ventiles_df['Interval'] = pd.qcut(ventiles_df['Probability'], q = 20, labels = [number for number in range(1, 21)], precision = 0)\n",
    "ventiles_df['Ventile'] = pd.qcut(ventiles_df['Probability'], q = 20, precision = 0)\n",
    "ventiles_df = ventiles_df.groupby(['Interval', 'Ventile']).agg(agg_dict).dropna()\n",
    "ventiles_df['Odds'] = ventiles_df['Probability'] / (1 - ventiles_df['Probability'])\n",
    "ventiles_df['$1M+ Prod %'] = ventiles_df['$1M+ Prod'][::-1].cumsum()[::-1] / ventiles_df['$1M+ Prod'].sum() * 100\n",
    "display(ventiles_df)\n",
    "\n",
    "coefficients = pd.DataFrame({\n",
    "    'variable': ['intercept'] + list(np.array(X_train_collapsed_rules_indeterminate_set[collapsed_rules].columns)[grid_search_smote_collapsed_rules_indeterminate_set.best_estimator_['vt'].get_support()]),\n",
    "    'coefficient': [grid_search_smote_collapsed_rules_indeterminate_set.best_estimator_['logistic'].intercept_[0]] + list(grid_search_smote_collapsed_rules_indeterminate_set.best_estimator_['logistic'].coef_[0])\n",
    "}).merge(complexity_table[['Name', 'ComplexityRuleID']].astype(str).set_index('ComplexityRuleID'), how = 'left', left_on = 'variable', right_on = 'ComplexityRuleID')\n",
    "display(coefficients.sort_values('coefficient', ascending = False, key = abs).fillna('').head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (8, 5), dpi = 300)\n",
    "CalibrationDisplay.from_estimator(estimator = grid_search_smote_all_rules.best_estimator_, X = X_test_all_rules[all_rules], y = y_test_all_rules, n_bins = 20, strategy = 'quantile', name = 'Logistic LASSO SMOTE | All Rules', markersize = 3, ax = ax)\n",
    "CalibrationDisplay.from_estimator(estimator = grid_search_smote_all_rules_indeterminate_set.best_estimator_, X = X_test_all_rules_indeterminate_set[all_rules], y = y_test_all_rules_indeterminate_set, n_bins = 20, strategy = 'quantile',  name = 'Logistic LASSO SMOTE | All Rules | Indeterminate Set', markersize = 3, ax = ax)\n",
    "CalibrationDisplay.from_estimator(estimator = grid_search_smote_collapsed_rules.best_estimator_, X = X_test_collapsed_rules[collapsed_rules], y = y_test_collapsed_rules, n_bins = 20, strategy = 'quantile',  name = 'Logistic LASSO SMOTE | Collapsed Rules', markersize = 3, ax = ax)\n",
    "CalibrationDisplay.from_estimator(estimator = grid_search_smote_collapsed_rules_indeterminate_set.best_estimator_, X = X_test_collapsed_rules_indeterminate_set[collapsed_rules], y = y_test_collapsed_rules_indeterminate_set, n_bins = 20, strategy = 'quantile',  name = 'Logistic LASSO SMOTE | Collapsed Rules | Indeterminate Set', markersize = 3, ax = ax)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d6140ef0c675026b0200147df87972487ebc0097827c4c765c9e0dcd9cf7b2f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
